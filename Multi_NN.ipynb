{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "\n",
    "import textstat\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "#from transformers import AdamW\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "#from tensorflow_addons.optimizers import AdamW\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_01 = pd.read_csv('training_data.csv')\n",
    "print(len(df_tr_01))\n",
    "print(df_tr_01.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unte_01 = pd.read_csv('unlabelled_test_data.csv')\n",
    "print(len(df_unte_01))\n",
    "print(df_unte_01.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tr_01.shape)\n",
    "print(df_tr_01.info())\n",
    "print('----------')\n",
    "df_tr_02 = df_tr_01.copy()\n",
    "for i in range(0, len(df_tr_02.difficulty)):\n",
    "    if df_tr_02.iloc[i,2] == \"A1\":\n",
    "        df_tr_02.iloc[i,2] = int(1)\n",
    "    elif df_tr_02.iloc[i,2] == \"A2\":\n",
    "        df_tr_02.iloc[i,2] =int(2)\n",
    "    elif df_tr_02.iloc[i,2] == \"B1\":\n",
    "        df_tr_02.iloc[i,2] = int(3)\n",
    "    elif df_tr_02.iloc[i,2] == \"B2\":\n",
    "        df_tr_02.iloc[i,2] = int(4)\n",
    "    elif df_tr_02.iloc[i,2] == \"C1\":\n",
    "        df_tr_02.iloc[i,2] = int(5)\n",
    "    elif df_tr_02.iloc[i,2] == \"C2\":\n",
    "        df_tr_02.iloc[i,2] = int(6)\n",
    "df_tr_02.difficulty.unique()\n",
    "\n",
    "df_tr_05_02 = df_tr_01.copy()\n",
    "for i in range(0, len(df_tr_02.difficulty)):\n",
    "    if df_tr_05_02.iloc[i,2] == \"A1\":\n",
    "        df_tr_05_02.iloc[i,2] = int(0)\n",
    "    elif df_tr_05_02.iloc[i,2] == \"A2\":\n",
    "        df_tr_05_02.iloc[i,2] =int(1)\n",
    "    elif df_tr_05_02.iloc[i,2] == \"B1\":\n",
    "        df_tr_05_02.iloc[i,2] = int(2)\n",
    "    elif df_tr_05_02.iloc[i,2] == \"B2\":\n",
    "        df_tr_05_02.iloc[i,2] = int(3)\n",
    "    elif df_tr_05_02.iloc[i,2] == \"C1\":\n",
    "        df_tr_05_02.iloc[i,2] = int(4)\n",
    "    elif df_tr_05_02.iloc[i,2] == \"C2\":\n",
    "        df_tr_05_02.iloc[i,2] = int(5)\n",
    "df_tr_05_02.difficulty.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize French stemmer\n",
    "stemmer = SnowballStemmer(\"french\")\n",
    "\n",
    "# French stopwords from NLTK\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "\n",
    "def clean_text_french(text):\n",
    "    # Clean and prepare text\n",
    "    text = re.sub(\"[^\\w\\s]\", \" \", text)\n",
    "    # Split text into words, filter stopwords, apply stemming, and convert to lower case\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split() if word.lower() not in french_stopwords]).lower()\n",
    "\n",
    "'''\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'text': [\"Café au centre de Paris\", \"Il fait un beau jour!\", \"résumé et portfolio\"]\n",
    "})\n",
    "'''\n",
    "df_tr_03 = df_tr_02.copy()\n",
    "\n",
    "# Applying the French text cleaning function\n",
    "df_tr_03['cleaned'] = df_tr_03['sentence'].apply(clean_text_french)\n",
    "print(df_tr_03.cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load French Spacy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def clean_text_french_lemmatized(text):\n",
    "    # Process text using Spacy\n",
    "    doc = nlp(text)\n",
    "    # Filter stopwords and punctuation, then lemmatize\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct]).lower()\n",
    "\n",
    "df_tr_04 = df_tr_02.copy()\n",
    "\n",
    "# Applying the French text cleaning function with lemmatization\n",
    "df_tr_04['cleaned'] = df_tr_04['sentence'].apply(clean_text_french_lemmatized)\n",
    "print(df_tr_04.cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camembert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camembert embeddings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_tr_20 = df_tr_02.copy()\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "model = CamembertModel.from_pretrained('camembert-base')\n",
    "\n",
    "def get_embeddings(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    encoded_input = {key: val.to(model.device) for key, val in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    #hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    embeddings = embeddings.mean(dim=1)\n",
    "    return embeddings.squeeze().cpu().tolist()\n",
    "\n",
    "df_tr_20['embeddings'] = df_tr_20['sentence'].apply(get_embeddings)\n",
    "#print(df_tr_20['embeddings'].shape())\n",
    "df_tr_20.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tr_20['embeddings'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi input NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#si weight and biases account\n",
    "'''\n",
    "import wandb\n",
    "run = wandb.init(\n",
    "    project='Cam02', \n",
    "    name=\"001\", \n",
    ")\n",
    "'''\n",
    "\n",
    "df_tr_08 = df_tr_01.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "df_tr_08['difficulty'] = label_encoder.fit_transform(df_tr_08['difficulty'])\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "def extract_features(doc):\n",
    "    # Lexical features\n",
    "    words = [token.text for token in doc if token.is_alpha]\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "    type_token_ratio = len(set(words)) / len(words) if words else 0\n",
    "    \n",
    "    # Lexical density\n",
    "    content_pos_tags = {'NOUN', 'VERB', 'ADJ', 'ADV'}\n",
    "    content_words = [token for token in doc if token.pos_ in content_pos_tags]\n",
    "    lexical_density = len(content_words) / len(words) if words else 0\n",
    "    \n",
    "    # Syntactic features\n",
    "    sentences = list(doc.sents)\n",
    "    avg_sentence_length = sum(len(sentence) for sentence in sentences) / len(sentences) if sentences else 0\n",
    "    \n",
    "    #Syntactic complexity: estimated by counting finite verbs in each sentence\n",
    "    clauses_per_sentence = [sum(1 for token in sentence if token.tag_ in ['VERB', 'AUX']) for sentence in sentences]\n",
    "    avg_clauses_per_sentence = sum(clauses_per_sentence) / len(clauses_per_sentence) if clauses_per_sentence else 0\n",
    "    \n",
    "    noun_phrases = list(doc.noun_chunks)\n",
    "    avg_noun_phrase_length = sum(len(np) for np in noun_phrases) / len(noun_phrases) if noun_phrases else 0\n",
    "    \n",
    "    #Count of nouns, verbs, adjectives\n",
    "    num_nouns = sum(1 for token in doc if token.pos_ == 'NOUN')\n",
    "    num_verbs = sum(1 for token in doc if token.pos_ == 'VERB')\n",
    "    num_adjectives = sum(1 for token in doc if token.pos_ == 'ADJ')\n",
    "    \n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'type_token_ratio': type_token_ratio,\n",
    "        'lexical_density': lexical_density,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_clauses_per_sentence': avg_clauses_per_sentence,\n",
    "        'avg_noun_phrase_length': avg_noun_phrase_length,\n",
    "        'num_nouns': num_nouns,\n",
    "        'num_verbs': num_verbs,\n",
    "        'num_adjectives': num_adjectives\n",
    "    }\n",
    "\n",
    "def add_features_to_dataframe(df, text_column):\n",
    "    df['spacy_doc'] = df[text_column].apply(nlp)  # Convert text to spacy Doc objects\n",
    "    features_df = df['spacy_doc'].apply(extract_features).apply(pd.Series)\n",
    "    new_df = pd.concat([df, features_df], axis=1)\n",
    "    new_df.drop('spacy_doc', axis=1, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "df_with_features_01 = add_features_to_dataframe(df_tr_08, 'sentence')\n",
    "\n",
    "df_with_features_02 = df_with_features_01.copy()\n",
    "df_with_features_02['embeddings'] = df_tr_20['embeddings'] #df_tr_20 les embed camembert\n",
    "df_with_features_02 = df_with_features_02.drop('sentence', axis=1)\n",
    "df_with_features_02 = df_with_features_02.drop('id', axis=1)\n",
    "df_only_features_02 = df_with_features_02.drop('difficulty', axis=1)\n",
    "\n",
    "df_y_01 = df_tr_05_02.copy()\n",
    "df_y_01['difficulty'] = df_y_01['difficulty'].astype(float)\n",
    "print('----------------------------------------------')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_only_features_02, df_y_01['difficulty'], test_size=0.1, random_state=42)\n",
    "\n",
    "X_train_embeddings = X_train['embeddings'].tolist()\n",
    "X_train_features = X_train.drop('embeddings', axis=1)\n",
    "\n",
    "X_test_embeddings = X_test['embeddings'].tolist()\n",
    "X_test_features = X_test.drop('embeddings', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "X_test_scaled = scaler.fit_transform(X_test_features) ######################## transform\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_features.columns, index=X_train_features.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_features.columns, index=X_test_features.index)\n",
    "#---------------------------------------------------------\n",
    "#----------------------------------------------------------------\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest.fit(X_train_scaled, y_train)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "important_feature_indices = np.argsort(importances)[::-1][:4]  # Select top 3 features for simplicity\n",
    "\n",
    "important_features = X_train_scaled.columns[important_feature_indices]\n",
    "\n",
    "X_train_numerical_selected = X_train_scaled[important_features]\n",
    "X_test_numerical_selected = X_test_scaled[important_features]\n",
    "\n",
    "'''\n",
    "pca = PCA(n_components=800)  # Reduce to 50 components\n",
    "X_train_embeddings_reduced = pca.fit_transform(X_train_embeddings)\n",
    "X_test_embeddings_reduced = pca.transform(X_test_embeddings)\n",
    "'''\n",
    "#----------------------------------------------------------------\n",
    "'''\n",
    "numeric_input = Input(shape=(X_train_numerical_selected.shape[1],), name='numeric_data')\n",
    "embedding_input = Input(shape=(len(X_train_embeddings[0]),), name='embedding_data')\n",
    "'''\n",
    "\n",
    "print(\"Shape of X_train_numerical_selected:\", X_train_numerical_selected.shape)\n",
    "\n",
    "# Input layers\n",
    "input_numerical = Input(shape=(X_train_numerical_selected.shape[1],), name='numeric_input')\n",
    "#input_embeddings = Input(shape=(800,), name='embedding_input')\n",
    "input_embeddings = Input(shape=(np.array(X_train_embeddings).shape[1],), name='embedding_input')\n",
    "\n",
    "# Processing paths\n",
    "path_numerical = Dense(50, activation='relu')(input_numerical)\n",
    "path_embeddings = Dense(50, activation='relu')(input_embeddings)\n",
    "\n",
    "# Concatenate data paths\n",
    "concatenated = Concatenate()([path_numerical, path_embeddings])\n",
    "\n",
    "# Additional Dense layers\n",
    "dense = Dense(50, activation='relu')(concatenated)\n",
    "dense = Dense(25, activation='relu')(dense) \n",
    "#dropout = Dropout(0.5)(dense)\n",
    "output = Dense(6, activation='softmax')(dense)\n",
    "\n",
    "optimizer = AdamW(learning_rate=0.0001) # 0.01\n",
    "\n",
    "model = Model(inputs=[input_numerical, input_embeddings], outputs=output)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train_embeddings_reduced_np = np.array(X_train_embeddings)\n",
    "X_test_embeddings_reduced_np = np.array(X_test_embeddings)\n",
    "\n",
    "#print(\"Shape of X_train_numerical_selected:\", X_train_numerical_selected.shape)\n",
    "#print(\"Shape of X_train_embeddings_reduced:\", X_train_embeddings_reduced.shape)\n",
    "\n",
    "model.fit([X_train_numerical_selected, X_train_embeddings_reduced_np], y_train, epochs=8, validation_split=0.1, batch_size=1) #50 0.1 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate([(X_test_numerical_selected, X_test_embeddings_reduced_np)], y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare unabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save unte\n",
    "\n",
    "df_tr_un_08 = df_unte_01.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "#WIP use only the selected features\n",
    "'''\n",
    "def extract_features(doc):\n",
    "    # Lexical features\n",
    "    words = [token.text for token in doc if token.is_alpha]\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "    type_token_ratio = len(set(words)) / len(words) if words else 0\n",
    "    \n",
    "    # Lexical density: proportion of content words (nouns, verbs, adjectives, adverbs)\n",
    "    content_pos_tags = {'NOUN', 'VERB', 'ADJ', 'ADV'}\n",
    "    content_words = [token for token in doc if token.pos_ in content_pos_tags]\n",
    "    lexical_density = len(content_words) / len(words) if words else 0\n",
    "    \n",
    "    # Syntactic features\n",
    "    sentences = list(doc.sents)\n",
    "    avg_sentence_length = sum(len(sentence) for sentence in sentences) / len(sentences) if sentences else 0\n",
    "    \n",
    "    # Syntactic complexity: estimated by counting finite verbs in each sentence\n",
    "    clauses_per_sentence = [sum(1 for token in sentence if token.tag_ in ['VERB', 'AUX']) for sentence in sentences]\n",
    "    avg_clauses_per_sentence = sum(clauses_per_sentence) / len(clauses_per_sentence) if clauses_per_sentence else 0\n",
    "    \n",
    "    noun_phrases = list(doc.noun_chunks)\n",
    "    avg_noun_phrase_length = sum(len(np) for np in noun_phrases) / len(noun_phrases) if noun_phrases else 0\n",
    "    \n",
    "    # Semantic features: Count of nouns, verbs, adjectives\n",
    "    num_nouns = sum(1 for token in doc if token.pos_ == 'NOUN')\n",
    "    num_verbs = sum(1 for token in doc if token.pos_ == 'VERB')\n",
    "    num_adjectives = sum(1 for token in doc if token.pos_ == 'ADJ')\n",
    "    \n",
    "    # Return all features as a dictionary\n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'type_token_ratio': type_token_ratio,\n",
    "        'lexical_density': lexical_density,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_clauses_per_sentence': avg_clauses_per_sentence,\n",
    "        'avg_noun_phrase_length': avg_noun_phrase_length,\n",
    "        'num_nouns': num_nouns,\n",
    "        'num_verbs': num_verbs,\n",
    "        'num_adjectives': num_adjectives\n",
    "    }\n",
    "'''\n",
    "def add_features_to_dataframe(df, text_column):\n",
    "    df['spacy_doc'] = df[text_column].apply(nlp)\n",
    "    features_df = df['spacy_doc'].apply(extract_features).apply(pd.Series)\n",
    "    new_df = pd.concat([df, features_df], axis=1)\n",
    "    new_df.drop('spacy_doc', axis=1, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "df_un_with_features_01 = add_features_to_dataframe(df_tr_un_08, 'sentence')\n",
    "df_un_with_features_02 = df_un_with_features_01.copy()\n",
    "df_un_with_features_02['embeddings'] = df_tr_20['embeddings']\n",
    "df_un_with_features_02 = df_un_with_features_02.drop('sentence', axis=1)\n",
    "df_un_with_features_02 = df_un_with_features_02.drop('id', axis=1)\n",
    "\n",
    "X_train_embeddings = df_un_with_features_02['embeddings'].tolist()\n",
    "X_train_features = df_un_with_features_02.drop('embeddings', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_features.columns, index=X_train_features.index)\n",
    "'''\n",
    "# Get feature importance and select the most important ones\n",
    "importances = forest.feature_importances_\n",
    "important_feature_indices = np.argsort(importances)[::-1][:3]  # Select top N features\n",
    "\n",
    "important_features = X_train_scaled.columns[important_feature_indices]\n",
    "\n",
    "# Filter your train and test sets\n",
    "X_train_numerical_selected = X_train_scaled[:, important_feature_indices]\n",
    "X_test_numerical_selected = X_test_scaled[:, important_feature_indices]\n",
    "'''\n",
    "# Get feature importance and sort them\n",
    "#importances = forest.feature_importances_\n",
    "#important_feature_indices = np.argsort(importances)[::-1][:4]  # Select top 3 features for simplicity\n",
    "\n",
    "important_features = X_train_scaled.columns[important_feature_indices]\n",
    "\n",
    "X_train_numerical_selected = X_train_scaled[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_h5 = tf.keras.models.load_model('my_model.h5')\n",
    "predictions = loaded_model_h5.predict([X_new_numerical, X_new_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trainer\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "difficulty_levels = {0: 'A1', 1: 'A2', 2: 'B1', 3: 'B2', 4: 'C1', 5: 'C2'}\n",
    "predicted_difficulties = [difficulty_levels[label] for label in predicted_labels]\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': unlabelled_data_pd['id'],\n",
    "    'difficulty': predicted_difficulties\n",
    "})\n",
    "\n",
    "submission_df.to_csv('F:\\projects\\Matthias\\proj_01\\cam01\\submission_CamemBERT_018.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selected features:\", important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_numerical_selected.shape, np.array(X_train_embeddings).shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_numerical_selected.shape, np.array(X_test_embeddings).shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "def apply_pca(X, n_components=666): #50\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca\n",
    "'''\n",
    "\n",
    "class FullHyperModel(HyperModel):\n",
    "    def __init__(self, X_train_num, X_train_emb, y_train):\n",
    "        self.X_train_num = X_train_num\n",
    "        self.X_train_emb = X_train_emb\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def build(self, hp):\n",
    "        # PCA for embeddings\n",
    "        #n_components = hp.Int('n_components', min_value=100, max_value=1000, step=20)\n",
    "        \n",
    "        '''\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_train_emb_pca = pca.fit_transform(self.X_train_emb)\n",
    "        '''\n",
    "\n",
    "    \n",
    "        input_emb = Input(shape=(self.X_train_emb.shape[1],), name='input_emb')\n",
    "        emb_units = hp.Int('emb_units', min_value=20, max_value=256, step=32)\n",
    "        emb_path = Dense(emb_units, activation='relu')(input_emb)\n",
    "\n",
    "        # Input for numerical features\n",
    "        input_num = Input(shape=(self.X_train_num.shape[1],), name='input_num')\n",
    "        num_units = hp.Int('num_units', min_value=20, max_value=256, step=32)\n",
    "        num_path = Dense(num_units, activation='relu')(input_num)\n",
    "\n",
    "        # Concatenate and further processing\n",
    "        concatenated = Concatenate()([num_path, emb_path])\n",
    "        final_units = hp.Int('final_units', min_value=10, max_value=256, step=32)\n",
    "        dense = Dense(final_units, activation='relu')(concatenated)\n",
    "        \n",
    "        output = Dense(6, activation='softmax')(dense)  \n",
    "        \n",
    "        # Hyperparameters for the optimizer\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "        weight_decay = hp.Float('weight_decay', min_value=1e-5, max_value=1e-2, step=1e-5)\n",
    "\n",
    "        # Model compilation\n",
    "        model = Model(inputs=[input_num, input_emb], outputs=output)\n",
    "        '''\n",
    "        model.compile(\n",
    "            optimizer=AdamW(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]),weight_decay=hp.Float('weight_decay', min_value=0.01, max_value=0.1, step=0.01)),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        '''\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay),\n",
    "            # optimizer=AdamW(learning_rate=learning_rate, weight_decay=weight_decay),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        batch_size = hp.Int('batch_size', min_value=1, max_value=16)\n",
    "        #n_components = hp.get('n_components')\n",
    "        #pca = PCA(n_components=n_components)\n",
    "        #X_train_emb_pca = pca.fit_transform(self.X_train_emb)\n",
    "        print(\"Training data shape:\", self.X_train_num.shape)\n",
    "        print(\"Training embeddings shape:\", self.X_train_emb.shape)\n",
    "        print(\"Labels shape:\", self.y_train.shape)\n",
    "        print(\"batch size:\", batch_size)\n",
    "        return model.fit([self.X_train_num, self.X_train_emb], self.y_train, batch_size=batch_size, *args, **kwargs)\n",
    "    \n",
    "tuner = RandomSearch(\n",
    "    FullHyperModel(X_train_numerical_selected, np.array(X_train_embeddings), y_train),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=100, \n",
    "    executions_per_trial=4, \n",
    "    directory='my_tuning_dir',\n",
    "    project_name='full_model_tuning_11'\n",
    ")\n",
    "\n",
    "tuner.search(epochs=30, validation_split=0.1)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.save('best_model_03.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "batch_size = best_hp.get('batch_size')\n",
    "learning_rate = best_hp.get('learning_rate')\n",
    "weight_decay = best_hp.get('weight_decay')\n",
    "\n",
    "\n",
    "# Optionally, directly fetch the best model using the best hyperparameters\n",
    "#best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "hyperparameters = {\n",
    "    'batch_size': best_hp.get('batch_size'),\n",
    "    'learning_rate': best_hp.get('learning_rate'),\n",
    "    'weight_decay': best_hp.get('weight_decay'),\n",
    "    #'n_components': best_hp.get('n_components')\n",
    "}\n",
    "\n",
    "with open('best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(hyperparameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_numerical_selected.shape)\n",
    "print(len(X_train_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model_03.h5')\n",
    "# Load the hyperparameters\n",
    "with open('best_hyperparameters.json', 'r') as f:\n",
    "    hyperparams = json.load(f)\n",
    "\n",
    "print(hyperparams['batch_size'], hyperparams['learning_rate'], hyperparams['weight_decay'])\n",
    "\n",
    "#pca = PCA(n_components=700)  # Reduce to 50 components\n",
    "#X_test_embeddings_reduced = pca.fit_transform(X_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(X_test_numerical_selected, X_test_embeddings, y_test)\n",
    "test_loss, test_accuracy = model.evaluate([(X_test_numerical_selected, pd.DataFrame(X_test_embeddings))], y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "predictions = best_model.predict([X_test_numerical_selected, pd.DataFrame(X_test_embeddings)])\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "difficulty_levels = {0: 'A1', 1: 'A2', 2: 'B1', 3: 'B2', 4: 'C1', 5: 'C2'}\n",
    "predicted_difficulties = [difficulty_levels[label] for label in predicted_classes]\n",
    "original_difficulties = [difficulty_levels[label] for label in y_test]\n",
    "\n",
    "cm = confusion_matrix(original_difficulties, predicted_difficulties, labels=[\"A1\",\"A2\",\"B1\",\"B2\",\"C1\",\"C2\"])\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"A1\",\"A2\",\"B1\",\"B2\",\"C1\",\"C2\"], yticklabels=[\"A1\",\"A2\",\"B1\",\"B2\",\"C1\",\"C2\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
